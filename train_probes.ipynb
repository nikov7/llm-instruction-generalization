{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initial setup"
      ],
      "metadata": {
        "id": "FsAIWJ7z3pP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HQDqaXzj3M1k",
        "outputId": "2aa24ea7-29f8-45d2-f2eb-9137ea3f109d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-instruction-generalization'...\n",
            "remote: Enumerating objects: 34, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 34 (delta 13), reused 34 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (34/34), 456.82 KiB | 1.50 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nikov7/llm-instruction-generalization.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/llm-instruction-generalization/* /content\n",
        "!rm -rf /content/llm-instruction-generalization/"
      ],
      "metadata": {
        "id": "IX6pPFEV3TXQ",
        "outputId": "b04641a1-74d8-4579-a204-0d86c68f4d47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/llm-instruction-generalization/*': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation generation\n",
        "\n",
        "This part is required for the linear probe tests. Average time is ~3 hours per dataset, totalling to around ~9 hours.\n",
        "\n",
        "After running, a folder called `activations` will be created in each of these locations:\n",
        "\n",
        "```\n",
        "data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v1\n",
        "\n",
        "data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v2\n",
        "\n",
        "data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v3\n",
        "```\n"
      ],
      "metadata": {
        "id": "qHmJ-TPv3wWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before running the code below, `save_activations.sh` must be modified to include your personal HuggingFace token."
      ],
      "metadata": {
        "id": "ZvGZ3HOu9ft6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sh save_activations.sh"
      ],
      "metadata": {
        "id": "ooEyTTmd4BbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Alternative (Time saving)\n",
        "To avoid running the script for many hours, I have uploaded the activation folders for each dataset in three separate files. They must be copied into the data folder.\n",
        "\n",
        "They can be retrieved at:\n",
        "https://github.com/nikov7/llm-instruction-generalization/releases/tag/release\n"
      ],
      "metadata": {
        "id": "dGMsw_Nj91Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the IFeval checker\n",
        "\n",
        "This comes after the activations have been generated. It creates a file called `eval_results_loose.jsonl` in the data folder for each dataset."
      ],
      "metadata": {
        "id": "lx4UK1X54CMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install absl-py langdetect immutabledict"
      ],
      "metadata": {
        "id": "JYGfqRYV-y2e",
        "outputId": "465e1f89-a7c0-4cd8-b788-8c270bd9ffd1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m921.6/981.5 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (4.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=3b80b53ad9ca9635c655621e7c0545c2a182209c89290d3b5c16057efe3c6c43\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sh ifeval_run.sh"
      ],
      "metadata": {
        "id": "vUgUP5un-gWP",
        "outputId": "61a3d937-1a16-40d5-ce45-08b07a744892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I0716 16:05:08.377763 131979149693568 evaluation_main.py:57] Generating eval_results_strict...\n",
            "I0716 16:05:08.400919 131979149693568 evaluation_main.py:63] Accuracy: 0.260784\n",
            "I0716 16:05:08.414464 131979149693568 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v1/eval_results_strict.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v1/eval_results_strict.jsonl Accuracy Scores:\n",
            "prompt-level: 0.2607843137254902\n",
            "instruction-level: 0.2607843137254902\n",
            "\n",
            "detectable_content 0.45098039215686275\n",
            "keywords 0.24509803921568626\n",
            "startend 0.11764705882352941\n",
            "\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.12745098039215685\n",
            "keywords:frequency 0.3431372549019608\n",
            "startend:end_checker 0.11764705882352941\n",
            "I0716 16:05:08.415240 131979149693568 evaluation_main.py:57] Generating eval_results_loose...\n",
            "I0716 16:05:08.495729 131979149693568 evaluation_main.py:63] Accuracy: 0.317647\n",
            "I0716 16:05:08.551111 131979149693568 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v1/eval_results_loose.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v1/eval_results_loose.jsonl Accuracy Scores:\n",
            "prompt-level: 0.3176470588235294\n",
            "instruction-level: 0.3176470588235294\n",
            "\n",
            "detectable_content 0.45098039215686275\n",
            "keywords 0.3333333333333333\n",
            "startend 0.13725490196078433\n",
            "\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.39215686274509803\n",
            "keywords:frequency 0.3431372549019608\n",
            "startend:end_checker 0.13725490196078433\n",
            "I0716 16:05:09.932351 138334878511744 evaluation_main.py:57] Generating eval_results_strict...\n",
            "I0716 16:05:10.011209 138334878511744 evaluation_main.py:63] Accuracy: 0.238971\n",
            "I0716 16:05:10.032292 138334878511744 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v2/eval_results_strict.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v2/eval_results_strict.jsonl Accuracy Scores:\n",
            "prompt-level: 0.23897058823529413\n",
            "instruction-level: 0.23897058823529413\n",
            "\n",
            "change_case 0.0\n",
            "detectable_content 0.45098039215686275\n",
            "detectable_format 0.10784313725490197\n",
            "keywords 0.24509803921568626\n",
            "length_constraints 0.5\n",
            "startend 0.11764705882352941\n",
            "\n",
            "change_case:english_lowercase 0.0\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "detectable_format:number_bullet_lists 0.10784313725490197\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.12745098039215685\n",
            "keywords:frequency 0.3431372549019608\n",
            "length_constraints:number_words 0.5\n",
            "startend:end_checker 0.11764705882352941\n",
            "I0716 16:05:10.033517 138334878511744 evaluation_main.py:57] Generating eval_results_loose...\n",
            "I0716 16:05:12.269307 138334878511744 evaluation_main.py:63] Accuracy: 0.280637\n",
            "I0716 16:05:12.290545 138334878511744 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v2/eval_results_loose.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v2/eval_results_loose.jsonl Accuracy Scores:\n",
            "prompt-level: 0.2806372549019608\n",
            "instruction-level: 0.2806372549019608\n",
            "\n",
            "change_case 0.0\n",
            "detectable_content 0.45098039215686275\n",
            "detectable_format 0.12745098039215685\n",
            "keywords 0.3333333333333333\n",
            "length_constraints 0.5294117647058824\n",
            "startend 0.13725490196078433\n",
            "\n",
            "change_case:english_lowercase 0.0\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "detectable_format:number_bullet_lists 0.12745098039215685\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.39215686274509803\n",
            "keywords:frequency 0.3431372549019608\n",
            "length_constraints:number_words 0.5294117647058824\n",
            "startend:end_checker 0.13725490196078433\n",
            "I0716 16:05:13.765068 137257912029824 evaluation_main.py:57] Generating eval_results_strict...\n",
            "I0716 16:05:14.150460 137257912029824 evaluation_main.py:63] Accuracy: 0.212418\n",
            "I0716 16:05:14.174220 137257912029824 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v3/eval_results_strict.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v3/eval_results_strict.jsonl Accuracy Scores:\n",
            "prompt-level: 0.21241830065359477\n",
            "instruction-level: 0.21274509803921568\n",
            "\n",
            "change_case 0.05392156862745098\n",
            "detectable_content 0.45098039215686275\n",
            "detectable_format 0.11274509803921569\n",
            "keywords 0.24509803921568626\n",
            "length_constraints 0.49019607843137253\n",
            "startend 0.11764705882352941\n",
            "\n",
            "change_case:english_lowercase 0.05392156862745098\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "detectable_format:number_bullet_lists 0.11274509803921569\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.12745098039215685\n",
            "keywords:frequency 0.3431372549019608\n",
            "length_constraints:number_words 0.49019607843137253\n",
            "startend:end_checker 0.11764705882352941\n",
            "I0716 16:05:14.175613 137257912029824 evaluation_main.py:57] Generating eval_results_loose...\n",
            "I0716 16:05:16.320232 137257912029824 evaluation_main.py:63] Accuracy: 0.248366\n",
            "I0716 16:05:16.344027 137257912029824 evaluation_main.py:69] Generated: ./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v3/eval_results_loose.jsonl\n",
            "================================================================\n",
            "./data/TinyLlama-1.1B-Chat-v1.0/ifeval_simple_v3/eval_results_loose.jsonl Accuracy Scores:\n",
            "prompt-level: 0.24836601307189543\n",
            "instruction-level: 0.246078431372549\n",
            "\n",
            "change_case 0.06372549019607843\n",
            "detectable_content 0.45098039215686275\n",
            "detectable_format 0.12254901960784313\n",
            "keywords 0.3333333333333333\n",
            "length_constraints 0.5\n",
            "startend 0.13725490196078433\n",
            "\n",
            "change_case:english_lowercase 0.06372549019607843\n",
            "detectable_content:number_placeholders 0.45098039215686275\n",
            "detectable_format:number_bullet_lists 0.12254901960784313\n",
            "keywords:existence 0.2647058823529412\n",
            "keywords:forbidden_words 0.39215686274509803\n",
            "keywords:frequency 0.3431372549019608\n",
            "length_constraints:number_words 0.5\n",
            "startend:end_checker 0.13725490196078433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probing\n",
        "\n",
        "This section is for the final probing."
      ],
      "metadata": {
        "id": "nk5n2phH--qk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r87fiKKQqsLC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "import nltk.data\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch as t\n",
        "import pickle\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-v73vfqsLC"
      },
      "source": [
        "## Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qnfGcwuqsLD"
      },
      "outputs": [],
      "source": [
        "def readjsonl(datapath):\n",
        "    res = []\n",
        "    with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f.readlines():\n",
        "            res.append(json.loads(line))\n",
        "    return res\n",
        "\n",
        "# // Get all detailed instructions\n",
        "def get_inst_list(task_path_ifeval):\n",
        "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
        "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
        "    inst_list=[]\n",
        "    for i in instruction_id_list:\n",
        "        for j in i:\n",
        "            if j not in inst_list:\n",
        "                inst_list.append(j)\n",
        "    return inst_list\n",
        "\n",
        "# // Get all high level instructions\n",
        "def get_high_inst_list(task_path_ifeval):\n",
        "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
        "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
        "    inst_list=[]\n",
        "    for i in instruction_id_list:\n",
        "        for j in i:\n",
        "            j = j.split(':')[0]\n",
        "            if j not in inst_list:\n",
        "                inst_list.append(j)\n",
        "    return inst_list\n",
        "\n",
        "# // Get all task type\n",
        "def get_task_list(task_path_ifeval):\n",
        "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
        "    prompt_df = ifeval_eval_df['prompt']\n",
        "    task_list=[]\n",
        "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "    for prompt in prompt_df:\n",
        "        task = tokenizer.tokenize(prompt)[0]\n",
        "        if task not in task_list:\n",
        "            task_list.append(task)\n",
        "    return task_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvVE8iUzqsLD"
      },
      "source": [
        "# Linear Probes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MED-zH2hqsLD"
      },
      "outputs": [],
      "source": [
        "class DataModuleActIfevalSimple:\n",
        "    def __init__(self,\n",
        "                 ifeval_data_path,\n",
        "                 ifeval_eval_path,\n",
        "                 inst_list,\n",
        "                 task_list,\n",
        "                 layer=13,\n",
        "                 target_token='last',\n",
        "                 center=True,\n",
        "                 scale=False,\n",
        "                 ):\n",
        "        self.layer=layer\n",
        "\n",
        "        # // Load data\n",
        "        self.ifeval_data = self.load_response_df(ifeval_data_path)\n",
        "        ifeval_eval_df = self.load_response_df(os.path.join(ifeval_eval_path, 'eval_results_loose.jsonl'))\n",
        "\n",
        "\n",
        "        # // Select index by inst\n",
        "        inst_ind = []\n",
        "        for i in range(len(ifeval_eval_df)):\n",
        "            if ifeval_eval_df.iloc[i]['instruction_id_list'][0] in inst_list:\n",
        "                inst_ind.append(i)\n",
        "\n",
        "        # // Select index by task\n",
        "        task_ind = []\n",
        "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "        for i in range(len(ifeval_eval_df)):\n",
        "            prompt = ifeval_eval_df.iloc[i]['prompt']\n",
        "            task = tokenizer.tokenize(prompt)[0]\n",
        "            if task in task_list:\n",
        "                task_ind.append(i)\n",
        "\n",
        "        # // Select index intersection\n",
        "        select_ind = list(set(inst_ind) & set(task_ind))\n",
        "\n",
        "        # // Load acts and labels\n",
        "        self.labels = torch.tensor(ifeval_eval_df['follow_all_instructions'])[select_ind]\n",
        "        self.labels = self.labels.float()\n",
        "        self.acts = self.collect_acts(ifeval_eval_path, layer=self.layer, target_token=target_token, device='cuda', center=center, scale=scale, index_list=select_ind)\n",
        "        self.acts = self.acts.float()\n",
        "        self.data={}\n",
        "        self.data = self.acts, self.labels\n",
        "        print('Saved layers: ', self.saved_layers)\n",
        "\n",
        "    def load_response_df(self, task_path, type='loose'):\n",
        "        response_df = pd.DataFrame(self.readjsonl(task_path))\n",
        "        return response_df\n",
        "\n",
        "    def readjsonl(self, datapath):\n",
        "        res = []\n",
        "        with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f.readlines():\n",
        "                res.append(json.loads(line))\n",
        "        return res\n",
        "\n",
        "    def load_pickle(self, filename: str):\n",
        "        with open(filename, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def collect_acts(self, task_path, layer=13, target_token='last', device='cuda', center=True, scale=False, index_list=None):\n",
        "        \"\"\"\n",
        "        Collects activations from a dataset of statements, returns as a tensor of shape [n_activations, activation_dimension].\n",
        "        First token: [1, len_input, hidden_emb]\n",
        "        Last token: [1, 1, hidden_emb]\n",
        "        \"\"\"\n",
        "        act_path = os.path.join(task_path, \"activations\")\n",
        "        _num_act = len(os.listdir(act_path))\n",
        "        acts = []\n",
        "        print('num_act: ', _num_act)\n",
        "        for _idx in range(_num_act):\n",
        "            if index_list is not None and _idx in index_list:\n",
        "                act_file_name = os.path.join(act_path, f\"sample_{_idx}.pkl\")\n",
        "                act = self.load_pickle(act_file_name)\n",
        "                self.saved_layers = act[f'output_token_{target_token}'].keys()\n",
        "                act = act[f'output_token_{target_token}'][f'layer_{layer}']\n",
        "                act = act[:,-1] # <-- last of the first token, no problem for last token --> [1, hidden_emb]\n",
        "                acts.append(act)\n",
        "        acts = torch.cat(acts, dim=0).to(device)\n",
        "        if center:\n",
        "            acts = acts - torch.mean(acts, dim=0)\n",
        "        if scale:\n",
        "            acts = acts / torch.std(acts, dim=0)\n",
        "        return acts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jmUViTdqsLD"
      },
      "outputs": [],
      "source": [
        "\n",
        "class LRProbe(t.nn.Module):\n",
        "    def __init__(self, d_in, binary_threshold=0.5, **kwargs):\n",
        "        super().__init__()\n",
        "        self.net = t.nn.Sequential(\n",
        "            t.nn.Linear(d_in, 1, bias=False),\n",
        "            t.nn.Sigmoid()\n",
        "        )\n",
        "        self.binary_threshold = binary_threshold\n",
        "\n",
        "    def forward(self, x, iid=None):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "    def pred(self, x, iid=None, binary_threshold=None):\n",
        "        binary_threshold = binary_threshold if binary_threshold is not None else self.binary_threshold\n",
        "        return (self(x)>binary_threshold).float()\n",
        "\n",
        "    def probability(self, x, iid=None):\n",
        "        return self(x)\n",
        "\n",
        "    def from_data(acts, labels, lr=0.001, weight_decay=0.1, epochs=1000, device='cpu', class_weight_one=None, **kwargs):\n",
        "        acts, labels = acts.to(device), labels.to(device)\n",
        "        probe = LRProbe(acts.shape[-1]).to(device)\n",
        "\n",
        "        opt = t.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        for _ in range(epochs):\n",
        "            opt.zero_grad()\n",
        "            if class_weight_one is not None:\n",
        "                class_weight = torch.ones_like(labels)\n",
        "                class_weight[labels>0] = class_weight_one\n",
        "                loss = t.nn.BCELoss(weight=class_weight)(probe(acts), labels)\n",
        "            else:\n",
        "                loss = t.nn.BCELoss()(probe(acts), labels)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        return probe\n",
        "\n",
        "    @property\n",
        "    def direction(self):\n",
        "        return self.net[0].weight.data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTivbrumqsLE"
      },
      "source": [
        "# 1. Task generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLhcQDSI3M1m"
      },
      "outputs": [],
      "source": [
        "def evaluate_task_generalization(ifeval_data_path, task_path_ifeval):\n",
        "    roc_list=[]\n",
        "    m_roc_list=[]\n",
        "    seed_list = np.random.randint(0, 10000, 5)\n",
        "    for seed in seed_list:\n",
        "        print(seed)\n",
        "\n",
        "        # // Select train and test task\n",
        "        task_list = np.array(get_task_list(ifeval_data_path))\n",
        "        torch.manual_seed(seed)\n",
        "        split=0.8\n",
        "        train_ind_list = torch.randperm(len(task_list)) < int(split * len(task_list))\n",
        "        test_ind_list = ~train_ind_list\n",
        "        train_task_list = task_list[train_ind_list]\n",
        "        test_task_list = task_list[test_ind_list]\n",
        "\n",
        "        # // Use all instructions\n",
        "        inst_list = get_inst_list(ifeval_data_path)\n",
        "\n",
        "        # // Get train data\n",
        "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, train_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, test_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        train_acts, train_labels = train_dm.data\n",
        "        test_acts, test_labels = test_dm.data\n",
        "\n",
        "        # // Scale and Center\n",
        "        all_acts = torch.cat((train_acts, test_acts))\n",
        "        print(all_acts.shape)\n",
        "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
        "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
        "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
        "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
        "\n",
        "        # // Stat of test\n",
        "        succ = (test_labels==1).sum()\n",
        "        fail = (test_labels==0).sum()\n",
        "        print('succ: ', succ)\n",
        "        print('fail: ', fail)\n",
        "\n",
        "\n",
        "        # // Train probe\n",
        "        max_roc=0\n",
        "        probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5)\n",
        "\n",
        "        # // Test\n",
        "        test_prob = probe.probability(test_acts).detach().cpu()\n",
        "        auroc = roc_auc_score(test_labels, test_prob)\n",
        "        roc_list.append(auroc)\n",
        "\n",
        "        print(LRProbe, ': ', auroc)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DpIszQZv3M1n"
      },
      "outputs": [],
      "source": [
        "LAYER=14\n",
        "MODEL='TinyLlama-1.1B-Chat-v1.0'\n",
        "TOKEN='first'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6T2vXHx3M1n"
      },
      "source": [
        "### 1.1 ifeval_simple (v1, original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1pSMrZ_3M1n",
        "outputId": "4cc1bbca-9ad0-4ec5-d749-36e3e6448e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3167\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(33)\n",
            "fail:  tensor(72)\n",
            "<class '__main__.LRProbe'> :  0.6593013468013468\n",
            "\n",
            "3857\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(31)\n",
            "fail:  tensor(74)\n",
            "<class '__main__.LRProbe'> :  0.6556233653007847\n",
            "\n",
            "2416\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(38)\n",
            "fail:  tensor(67)\n",
            "<class '__main__.LRProbe'> :  0.6441476826394344\n",
            "\n",
            "7642\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(32)\n",
            "fail:  tensor(73)\n",
            "<class '__main__.LRProbe'> :  0.7367294520547946\n",
            "\n",
            "7372\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(46)\n",
            "fail:  tensor(59)\n",
            "<class '__main__.LRProbe'> :  0.6879145173176123\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
        "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41pQebTy3M1n"
      },
      "source": [
        "### 1.2 ifeval_simple_v2 (3 new tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SphylTnF3M1n",
        "outputId": "e0942335-1f88-4ae8-ca26-ca1b459bbd7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3729\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(44)\n",
            "fail:  tensor(124)\n",
            "<class '__main__.LRProbe'> :  0.5706561583577713\n",
            "\n",
            "6878\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(57)\n",
            "fail:  tensor(111)\n",
            "<class '__main__.LRProbe'> :  0.5730994152046784\n",
            "\n",
            "7122\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(48)\n",
            "fail:  tensor(120)\n",
            "<class '__main__.LRProbe'> :  0.5078125\n",
            "\n",
            "2143\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(47)\n",
            "fail:  tensor(121)\n",
            "<class '__main__.LRProbe'> :  0.5657640232108317\n",
            "\n",
            "4523\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(48)\n",
            "fail:  tensor(120)\n",
            "<class '__main__.LRProbe'> :  0.5498263888888889\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
        "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP-vNDux3M1o"
      },
      "source": [
        "### 1.3 ifeval_simple_v3 (3 new tasks+combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeXVw6X03M1o",
        "outputId": "1354e712-40ec-4307-bc12-42527051c3fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4806\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(46)\n",
            "fail:  tensor(143)\n",
            "<class '__main__.LRProbe'> :  0.5799635147461235\n",
            "\n",
            "5161\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(57)\n",
            "fail:  tensor(132)\n",
            "<class '__main__.LRProbe'> :  0.583599149388623\n",
            "\n",
            "2413\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(52)\n",
            "fail:  tensor(137)\n",
            "<class '__main__.LRProbe'> :  0.592363840539023\n",
            "\n",
            "1951\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(37)\n",
            "fail:  tensor(152)\n",
            "<class '__main__.LRProbe'> :  0.5545874822190612\n",
            "\n",
            "916\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(49)\n",
            "fail:  tensor(140)\n",
            "<class '__main__.LRProbe'> :  0.47536443148688046\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
        "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGWFvQJbqsLE"
      },
      "source": [
        "# 2. Intruction generalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pH_Q6xAx3M1o"
      },
      "outputs": [],
      "source": [
        "def evaluate_inst_generalization(ifeval_data_path, task_path_ifeval, keyword_list):\n",
        "    # // Make a dict for result\n",
        "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
        "    re={}\n",
        "    all_label={}\n",
        "    all_pred={}\n",
        "    for i in inst_list:\n",
        "        if i not in re.keys():\n",
        "            re[i]=[]\n",
        "            all_label[i]=[]\n",
        "            all_pred[i]=[]\n",
        "    roc_list=[]\n",
        "    total_pred=[]\n",
        "    total_label=[]\n",
        "\n",
        "    # // Use all task\n",
        "    task_list = get_task_list(ifeval_data_path)\n",
        "\n",
        "    # // Select train and test inst\n",
        "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
        "\n",
        "    final={}\n",
        "    for inst in inst_list:\n",
        "        final[inst]=[]\n",
        "\n",
        "    for inst in inst_list:\n",
        "\n",
        "        # // Leave one out\n",
        "        train_inst_list = [i for i in keyword_list if i != inst]\n",
        "        test_inst_list = [inst]\n",
        "        print(train_inst_list)\n",
        "        print(test_inst_list)\n",
        "\n",
        "        # // Get train data\n",
        "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, train_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, test_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        train_acts, train_labels = train_dm.data\n",
        "        test_acts, test_labels = test_dm.data\n",
        "\n",
        "        # // Scale and Center\n",
        "        all_acts = torch.cat((train_acts, test_acts))\n",
        "        print(all_acts.shape)\n",
        "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
        "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
        "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
        "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
        "\n",
        "        # // Stat of test\n",
        "        succ = (test_labels==1).sum()\n",
        "        fail = (test_labels==0).sum()\n",
        "        print('te_succ: ', succ)\n",
        "        print('te_fail: ', fail)\n",
        "\n",
        "        # // Stat of train\n",
        "        tr_succ = (train_labels==1).sum()\n",
        "        tr_fail = (train_labels==0).sum()\n",
        "        print('tr_succ: ', tr_succ)\n",
        "        print('tr_fail: ', tr_fail)\n",
        "        tr_class_weight = tr_succ/tr_fail\n",
        "\n",
        "        # // exception\n",
        "        if succ<1 or fail<1:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # // Train probe\n",
        "        probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5, class_weight_one=None)\n",
        "\n",
        "        # // Test\n",
        "        test_prob = probe.probability(test_acts).detach().cpu()\n",
        "        auroc = roc_auc_score(test_labels, test_prob)\n",
        "\n",
        "        print(LRProbe, ': ', auroc)\n",
        "        print()\n",
        "\n",
        "        # // save\n",
        "        roc_list.append(auroc)\n",
        "        re[inst].append(auroc)\n",
        "        all_label[inst].append(test_labels)\n",
        "        all_pred[inst].append(test_prob)\n",
        "        total_label.append(test_labels)\n",
        "        total_pred.append(test_prob)\n",
        "\n",
        "    for key in all_pred.keys():\n",
        "        if len(all_pred[key])>0:\n",
        "            print(key)\n",
        "            label = np.concatenate(all_label[key])\n",
        "            pred = np.concatenate(all_pred[key])\n",
        "            final[key].append(roc_auc_score(label, pred ))\n",
        "\n",
        "    # // Compute all auc total\n",
        "    label = np.concatenate(total_label)\n",
        "    pred = np.concatenate(total_pred)\n",
        "    total_auroc = roc_auc_score(label, pred)\n",
        "    print(f\"Total AUROC: {total_auroc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKiM749f3M1o"
      },
      "source": [
        "### 2.1 ifeval_simple (v1, original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kc4UC6p6aXiU",
        "outputId": "780ec65f-b281-4299-e6e3-de3ef10d24f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(122)\n",
            "tr_fail:  tensor(286)\n",
            "<class '__main__.LRProbe'> :  0.4959677419354839\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:existence']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(135)\n",
            "tr_fail:  tensor(273)\n",
            "<class '__main__.LRProbe'> :  0.5520987654320988\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:frequency']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(127)\n",
            "tr_fail:  tensor(281)\n",
            "<class '__main__.LRProbe'> :  0.49339019189765465\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(116)\n",
            "tr_fail:  tensor(292)\n",
            "<class '__main__.LRProbe'> :  0.46059782608695654\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders']\n",
            "['startend:end_checker']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(148)\n",
            "tr_fail:  tensor(260)\n",
            "<class '__main__.LRProbe'> :  0.5125811688311688\n",
            "\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "Total AUROC: 0.4984390520788988\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\"\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
        "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyaR0gZu3M1o"
      },
      "source": [
        "### 2.2 ifeval_simple_v2 (3 new tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPY6WB693M1o",
        "outputId": "54ed28e9-6f1b-4ca5-f786-80c9ca00e34b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(183)\n",
            "tr_fail:  tensor(531)\n",
            "<class '__main__.LRProbe'> :  0.47298387096774186\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:existence']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(196)\n",
            "tr_fail:  tensor(518)\n",
            "<class '__main__.LRProbe'> :  0.4795061728395062\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:frequency']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(188)\n",
            "tr_fail:  tensor(526)\n",
            "<class '__main__.LRProbe'> :  0.5599147121535182\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(177)\n",
            "tr_fail:  tensor(537)\n",
            "<class '__main__.LRProbe'> :  0.46855590062111807\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['startend:end_checker']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(209)\n",
            "tr_fail:  tensor(505)\n",
            "<class '__main__.LRProbe'> :  0.48133116883116883\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_format:number_bullet_lists']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(13)\n",
            "te_fail:  tensor(89)\n",
            "tr_succ:  tensor(210)\n",
            "tr_fail:  tensor(504)\n",
            "<class '__main__.LRProbe'> :  0.4848746758859118\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
            "['length_constraints:number_words']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(48)\n",
            "te_fail:  tensor(54)\n",
            "tr_succ:  tensor(175)\n",
            "tr_fail:  tensor(539)\n",
            "<class '__main__.LRProbe'> :  0.5374228395061729\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
            "['change_case:english_lowercase']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(0)\n",
            "te_fail:  tensor(102)\n",
            "tr_succ:  tensor(223)\n",
            "tr_fail:  tensor(491)\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "detectable_format:number_bullet_lists\n",
            "length_constraints:number_words\n",
            "Total AUROC: 0.4971916012895801\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\",\n",
        "    \"detectable_format:number_bullet_lists\",\n",
        "    \"length_constraints:number_words\",\n",
        "    \"change_case:english_lowercase\"\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
        "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umreR60I3M1p"
      },
      "source": [
        "### 2.3 ifeval_simple_v3 (3 new tasks+combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4gSwkJt3M1p",
        "outputId": "3decff06-dff0-4660-c7dc-28bcc45caa9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(191)\n",
            "tr_fail:  tensor(625)\n",
            "<class '__main__.LRProbe'> :  0.5044354838709677\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:existence']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(204)\n",
            "tr_fail:  tensor(612)\n",
            "<class '__main__.LRProbe'> :  0.4780246913580247\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:frequency']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(196)\n",
            "tr_fail:  tensor(620)\n",
            "<class '__main__.LRProbe'> :  0.5053304904051172\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(185)\n",
            "tr_fail:  tensor(631)\n",
            "<class '__main__.LRProbe'> :  0.5069875776397516\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['startend:end_checker']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(217)\n",
            "tr_fail:  tensor(599)\n",
            "<class '__main__.LRProbe'> :  0.5008116883116883\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_format:number_bullet_lists']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(13)\n",
            "te_fail:  tensor(89)\n",
            "tr_succ:  tensor(218)\n",
            "tr_fail:  tensor(598)\n",
            "<class '__main__.LRProbe'> :  0.5570440795159897\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
            "['length_constraints:number_words']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(54)\n",
            "te_fail:  tensor(48)\n",
            "tr_succ:  tensor(177)\n",
            "tr_fail:  tensor(639)\n",
            "<class '__main__.LRProbe'> :  0.470679012345679\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
            "['change_case:english_lowercase']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(2)\n",
            "te_fail:  tensor(202)\n",
            "tr_succ:  tensor(229)\n",
            "tr_fail:  tensor(485)\n",
            "<class '__main__.LRProbe'> :  0.693069306930693\n",
            "\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "detectable_format:number_bullet_lists\n",
            "length_constraints:number_words\n",
            "change_case:english_lowercase\n",
            "Total AUROC: 0.4996912354990958\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\",\n",
        "    \"detectable_format:number_bullet_lists\",\n",
        "    \"length_constraints:number_words\",\n",
        "    \"change_case:english_lowercase\",\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
        "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afbpZRz_3M1p"
      },
      "source": [
        "# 3. MLP Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzBjZVfq3M1p"
      },
      "outputs": [],
      "source": [
        "class MLPProbe(t.nn.Module):\n",
        "    \"\"\"\n",
        "    A small multi‑layer perceptron probe:\n",
        "      - input dim → hidden_dim → hidden_dim → 1 → sigmoid\n",
        "    \"\"\"\n",
        "    def __init__(self, d_in, hidden_dim=512, n_hidden=2, binary_threshold=0.5, dropout=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_dim = d_in\n",
        "        for _ in range(n_hidden):\n",
        "            layers += [\n",
        "                t.nn.Linear(in_dim, hidden_dim),\n",
        "                t.nn.ReLU(),\n",
        "                t.nn.Dropout(dropout),\n",
        "            ]\n",
        "            in_dim = hidden_dim\n",
        "        layers += [t.nn.Linear(in_dim, 1, bias=False), t.nn.Sigmoid()]\n",
        "        self.net = t.nn.Sequential(*layers)\n",
        "        self.binary_threshold = binary_threshold\n",
        "\n",
        "    def forward(self, x, iid=None):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "    def pred(self, x, iid=None, binary_threshold=None):\n",
        "        thresh = binary_threshold if binary_threshold is not None else self.binary_threshold\n",
        "        return (self(x) > thresh).float()\n",
        "\n",
        "    def probability(self, x, iid=None):\n",
        "        return self(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_data(acts, labels,\n",
        "                  lr=1e-3,\n",
        "                  weight_decay=1e-2,\n",
        "                  epochs=500,\n",
        "                  device='cpu',\n",
        "                  hidden_dim=512,\n",
        "                  n_hidden=2,\n",
        "                  dropout=0.1):\n",
        "        \"\"\"\n",
        "        Train an MLPProbe on (acts, labels) and return the fitted probe.\n",
        "        \"\"\"\n",
        "        acts, labels = acts.to(device), labels.to(device)\n",
        "        probe = MLPProbe(acts.shape[-1], hidden_dim, n_hidden, dropout=dropout).to(device)\n",
        "        optimizer = t.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        loss_fn = t.nn.BCELoss()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            optimizer.zero_grad()\n",
        "            preds = probe(acts)\n",
        "            loss = loss_fn(preds, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            # optional: print every 100 iters\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                with torch.no_grad():\n",
        "                    auc = roc_auc_score(labels.cpu().numpy(), preds.detach().cpu().numpy())\n",
        "                print(f\"epoch {epoch+1}/{epochs}, loss {loss.item():.4f}, auroc {auc:.3f}\")\n",
        "        return probe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4S97yBc3M1p"
      },
      "outputs": [],
      "source": [
        "def evaluate_task_generalization_mlp(ifeval_data_path, task_path_ifeval):\n",
        "    roc_list=[]\n",
        "    m_roc_list=[]\n",
        "    seed_list = np.random.randint(0, 10000, 5)\n",
        "    for seed in seed_list:\n",
        "        print(seed)\n",
        "\n",
        "        # // Select train and test task\n",
        "        task_list = np.array(get_task_list(ifeval_data_path))\n",
        "        torch.manual_seed(seed)\n",
        "        split=0.8\n",
        "        train_ind_list = torch.randperm(len(task_list)) < int(split * len(task_list))\n",
        "        test_ind_list = ~train_ind_list\n",
        "        train_task_list = task_list[train_ind_list]\n",
        "        test_task_list = task_list[test_ind_list]\n",
        "\n",
        "        # // Use all instructions\n",
        "        inst_list = get_inst_list(ifeval_data_path)\n",
        "\n",
        "        # // Get train data\n",
        "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, train_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, test_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        train_acts, train_labels = train_dm.data\n",
        "        test_acts, test_labels = test_dm.data\n",
        "\n",
        "        # // Scale and Center\n",
        "        all_acts = torch.cat((train_acts, test_acts))\n",
        "        print(all_acts.shape)\n",
        "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
        "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
        "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
        "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
        "\n",
        "        # // Stat of test\n",
        "        succ = (test_labels==1).sum()\n",
        "        fail = (test_labels==0).sum()\n",
        "        print('succ: ', succ)\n",
        "        print('fail: ', fail)\n",
        "\n",
        "\n",
        "        # // Train probe\n",
        "        max_roc=0\n",
        "\n",
        "        probe = MLPProbe.from_data(\n",
        "            train_acts, train_labels,\n",
        "            device='cuda',\n",
        "            lr=0.001,\n",
        "            weight_decay=0.01,\n",
        "            epochs=500,\n",
        "            hidden_dim=512,\n",
        "            n_hidden=2,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # // Test\n",
        "        test_prob = probe.probability(test_acts).detach().cpu()\n",
        "        auroc = roc_auc_score(test_labels, test_prob)\n",
        "        roc_list.append(auroc)\n",
        "\n",
        "        print(LRProbe, ': ', auroc)\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVnimPND3M1p"
      },
      "outputs": [],
      "source": [
        "def evaluate_inst_generalization_mlp(ifeval_data_path, task_path_ifeval, keyword_list):\n",
        "    # // Make a dict for result\n",
        "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
        "    re={}\n",
        "    all_label={}\n",
        "    all_pred={}\n",
        "    for i in inst_list:\n",
        "        if i not in re.keys():\n",
        "            re[i]=[]\n",
        "            all_label[i]=[]\n",
        "            all_pred[i]=[]\n",
        "    roc_list=[]\n",
        "    total_pred=[]\n",
        "    total_label=[]\n",
        "\n",
        "    # // Use all task\n",
        "    task_list = get_task_list(ifeval_data_path)\n",
        "\n",
        "    # // Select train and test inst\n",
        "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
        "\n",
        "    final={}\n",
        "    for inst in inst_list:\n",
        "        final[inst]=[]\n",
        "\n",
        "    for inst in inst_list:\n",
        "\n",
        "        # // Leave one out\n",
        "        train_inst_list = [i for i in keyword_list if i != inst]\n",
        "        test_inst_list = [inst]\n",
        "        print(train_inst_list)\n",
        "        print(test_inst_list)\n",
        "\n",
        "        # // Get train data\n",
        "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, train_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, test_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
        "        train_acts, train_labels = train_dm.data\n",
        "        test_acts, test_labels = test_dm.data\n",
        "\n",
        "        # // Scale and Center\n",
        "        all_acts = torch.cat((train_acts, test_acts))\n",
        "        print(all_acts.shape)\n",
        "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
        "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
        "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
        "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
        "\n",
        "        # // Stat of test\n",
        "        succ = (test_labels==1).sum()\n",
        "        fail = (test_labels==0).sum()\n",
        "        print('te_succ: ', succ)\n",
        "        print('te_fail: ', fail)\n",
        "\n",
        "        # // Stat of train\n",
        "        tr_succ = (train_labels==1).sum()\n",
        "        tr_fail = (train_labels==0).sum()\n",
        "        print('tr_succ: ', tr_succ)\n",
        "        print('tr_fail: ', tr_fail)\n",
        "        tr_class_weight = tr_succ/tr_fail\n",
        "\n",
        "        # // exception\n",
        "        if succ<1 or fail<1:\n",
        "            continue\n",
        "\n",
        "\n",
        "        # // Train probe\n",
        "        probe = MLPProbe.from_data(\n",
        "            train_acts, train_labels,\n",
        "            device='cuda',\n",
        "            lr=0.001,\n",
        "            weight_decay=0.01,\n",
        "            epochs=500,\n",
        "            hidden_dim=512,\n",
        "            n_hidden=4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "        # // Test\n",
        "        test_prob = probe.probability(test_acts).detach().cpu()\n",
        "        auroc = roc_auc_score(test_labels, test_prob)\n",
        "\n",
        "        print(LRProbe, ': ', auroc)\n",
        "        print()\n",
        "\n",
        "        # // save\n",
        "        roc_list.append(auroc)\n",
        "        re[inst].append(auroc)\n",
        "        all_label[inst].append(test_labels)\n",
        "        all_pred[inst].append(test_prob)\n",
        "        total_label.append(test_labels)\n",
        "        total_pred.append(test_prob)\n",
        "\n",
        "    for key in all_pred.keys():\n",
        "        if len(all_pred[key])>0:\n",
        "            print(key)\n",
        "            label = np.concatenate(all_label[key])\n",
        "            pred = np.concatenate(all_pred[key])\n",
        "            final[key].append(roc_auc_score(label, pred ))\n",
        "\n",
        "    # // Compute all auc total\n",
        "    label = np.concatenate(total_label)\n",
        "    pred = np.concatenate(total_pred)\n",
        "    total_auroc = roc_auc_score(label, pred)\n",
        "    print(f\"Total AUROC: {total_auroc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qUxoZd3M1q"
      },
      "source": [
        "### 3.1 ifeval_simple (v1, original)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBmqn9mG3M1q",
        "outputId": "055d1ab0-7bf6-47b7-f0f9-5b9d9b85ec00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8845\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(28)\n",
            "fail:  tensor(77)\n",
            "epoch 100/500, loss 0.0015, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.7500000000000001\n",
            "\n",
            "7829\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(43)\n",
            "fail:  tensor(62)\n",
            "epoch 100/500, loss 0.0006, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.689047261815454\n",
            "\n",
            "6161\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(35)\n",
            "fail:  tensor(70)\n",
            "epoch 100/500, loss 0.0013, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.6248979591836735\n",
            "\n",
            "5264\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(25)\n",
            "fail:  tensor(80)\n",
            "epoch 100/500, loss 0.0008, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.712\n",
            "\n",
            "8272\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "succ:  tensor(29)\n",
            "fail:  tensor(76)\n",
            "epoch 100/500, loss 0.0010, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0024, auroc 1.000\n",
            "epoch 400/500, loss 0.0009, auroc 1.000\n",
            "epoch 500/500, loss 0.0004, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.691016333938294\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
        "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taZqncbi3M1q",
        "outputId": "4cb98f27-d81d-4ccf-ff92-fd4730ca038a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(122)\n",
            "tr_fail:  tensor(286)\n",
            "epoch 100/500, loss 0.0037, auroc 1.000\n",
            "epoch 200/500, loss 0.0001, auroc 1.000\n",
            "epoch 300/500, loss 0.2509, auroc 0.967\n",
            "epoch 400/500, loss 0.0486, auroc 0.998\n",
            "epoch 500/500, loss 0.2300, auroc 0.962\n",
            "<class '__main__.LRProbe'> :  0.43306451612903224\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:existence']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(135)\n",
            "tr_fail:  tensor(273)\n",
            "epoch 100/500, loss 0.2265, auroc 0.998\n",
            "epoch 200/500, loss 0.0016, auroc 1.000\n",
            "epoch 300/500, loss 0.0021, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.4849382716049383\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
            "['keywords:frequency']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(127)\n",
            "tr_fail:  tensor(281)\n",
            "epoch 100/500, loss 0.0425, auroc 1.000\n",
            "epoch 200/500, loss 0.0000, auroc 1.000\n",
            "epoch 300/500, loss 0.0000, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5872068230277186\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(116)\n",
            "tr_fail:  tensor(292)\n",
            "epoch 100/500, loss 0.0024, auroc 1.000\n",
            "epoch 200/500, loss 0.0000, auroc 1.000\n",
            "epoch 300/500, loss 0.0000, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.4343944099378882\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders']\n",
            "['startend:end_checker']\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  510\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([510, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(148)\n",
            "tr_fail:  tensor(260)\n",
            "epoch 100/500, loss 0.2582, auroc 0.981\n",
            "epoch 200/500, loss 0.0036, auroc 1.000\n",
            "epoch 300/500, loss 0.0040, auroc 1.000\n",
            "epoch 400/500, loss 0.0002, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5604707792207793\n",
            "\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "Total AUROC: 0.491130977721016\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\"\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
        "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOBWyvvD3M1q"
      },
      "source": [
        "### 3.2 ifeval_simple_v2 (3 new tasks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-MlWB0f3M1q",
        "outputId": "7761d37e-03ac-4e96-b410-113939fefd47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7866\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(51)\n",
            "fail:  tensor(117)\n",
            "epoch 100/500, loss 0.0013, auroc 1.000\n",
            "epoch 200/500, loss 0.0913, auroc 0.992\n",
            "epoch 300/500, loss 0.0013, auroc 1.000\n",
            "epoch 400/500, loss 0.0003, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5760013407072231\n",
            "\n",
            "5527\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(51)\n",
            "fail:  tensor(117)\n",
            "epoch 100/500, loss 0.0027, auroc 1.000\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.583375230434054\n",
            "\n",
            "9141\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(42)\n",
            "fail:  tensor(126)\n",
            "epoch 100/500, loss 0.0034, auroc 1.000\n",
            "epoch 200/500, loss 0.0003, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.6375661375661377\n",
            "\n",
            "4289\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(38)\n",
            "fail:  tensor(130)\n",
            "epoch 100/500, loss 0.0029, auroc 1.000\n",
            "epoch 200/500, loss 0.0006, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 12.3668, auroc 0.683\n",
            "<class '__main__.LRProbe'> :  0.4779352226720648\n",
            "\n",
            "5831\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "succ:  tensor(45)\n",
            "fail:  tensor(123)\n",
            "epoch 100/500, loss 0.5142, auroc 0.963\n",
            "epoch 200/500, loss 0.0014, auroc 1.000\n",
            "epoch 300/500, loss 0.0004, auroc 1.000\n",
            "epoch 400/500, loss 0.0002, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.6445347786811201\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
        "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlS2iSDw3M1q",
        "outputId": "7ad54d36-4aea-4e11-aeac-dd3a69aedab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(183)\n",
            "tr_fail:  tensor(531)\n",
            "epoch 100/500, loss 0.0885, auroc 0.997\n",
            "epoch 200/500, loss 0.0007, auroc 1.000\n",
            "epoch 300/500, loss 0.0033, auroc 1.000\n",
            "epoch 400/500, loss 0.0002, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.4721774193548387\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:existence']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(196)\n",
            "tr_fail:  tensor(518)\n",
            "epoch 100/500, loss 0.0189, auroc 1.000\n",
            "epoch 200/500, loss 0.0006, auroc 1.000\n",
            "epoch 300/500, loss 0.0003, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0050, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5802469135802468\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:frequency']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(188)\n",
            "tr_fail:  tensor(526)\n",
            "epoch 100/500, loss 0.0236, auroc 1.000\n",
            "epoch 200/500, loss 0.0003, auroc 1.000\n",
            "epoch 300/500, loss 0.0561, auroc 0.999\n",
            "epoch 400/500, loss 0.1073, auroc 0.992\n",
            "epoch 500/500, loss 0.0024, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5543710021321961\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(177)\n",
            "tr_fail:  tensor(537)\n",
            "epoch 100/500, loss 0.0021, auroc 1.000\n",
            "epoch 200/500, loss 0.0013, auroc 1.000\n",
            "epoch 300/500, loss 0.0008, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.48990683229813664\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['startend:end_checker']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(209)\n",
            "tr_fail:  tensor(505)\n",
            "epoch 100/500, loss 0.2139, auroc 0.993\n",
            "epoch 200/500, loss 0.0012, auroc 1.000\n",
            "epoch 300/500, loss 0.0954, auroc 0.997\n",
            "epoch 400/500, loss 0.0055, auroc 1.000\n",
            "epoch 500/500, loss 0.0007, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5714285714285714\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_format:number_bullet_lists']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(13)\n",
            "te_fail:  tensor(89)\n",
            "tr_succ:  tensor(210)\n",
            "tr_fail:  tensor(504)\n",
            "epoch 100/500, loss 0.0773, auroc 0.996\n",
            "epoch 200/500, loss 0.0001, auroc 1.000\n",
            "epoch 300/500, loss 0.0054, auroc 1.000\n",
            "epoch 400/500, loss 0.0001, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5432152117545377\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
            "['length_constraints:number_words']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(48)\n",
            "te_fail:  tensor(54)\n",
            "tr_succ:  tensor(175)\n",
            "tr_fail:  tensor(539)\n",
            "epoch 100/500, loss 0.0423, auroc 1.000\n",
            "epoch 200/500, loss 0.0242, auroc 1.000\n",
            "epoch 300/500, loss 0.0000, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5802469135802469\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
            "['change_case:english_lowercase']\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  816\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([816, 2048])\n",
            "te_succ:  tensor(0)\n",
            "te_fail:  tensor(102)\n",
            "tr_succ:  tensor(223)\n",
            "tr_fail:  tensor(491)\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "detectable_format:number_bullet_lists\n",
            "length_constraints:number_words\n",
            "Total AUROC: 0.4898806316385522\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\",\n",
        "    \"detectable_format:number_bullet_lists\",\n",
        "    \"length_constraints:number_words\",\n",
        "    \"change_case:english_lowercase\"\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
        "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtYLlqvT3M1r"
      },
      "source": [
        "### 3.3 ifeval_simple_v3 (3 new tasks+combination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj5ZMBkC3M1r",
        "outputId": "638ffb2f-a9b3-4849-e720-58d93515b716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2116\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(51)\n",
            "fail:  tensor(138)\n",
            "epoch 100/500, loss 0.0043, auroc 1.000\n",
            "epoch 200/500, loss 0.0357, auroc 1.000\n",
            "epoch 300/500, loss 0.0026, auroc 1.000\n",
            "epoch 400/500, loss 0.0006, auroc 1.000\n",
            "epoch 500/500, loss 0.0003, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.6445012787723785\n",
            "\n",
            "9502\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(43)\n",
            "fail:  tensor(146)\n",
            "epoch 100/500, loss 0.0030, auroc 1.000\n",
            "epoch 200/500, loss 0.0003, auroc 1.000\n",
            "epoch 300/500, loss 0.1338, auroc 0.987\n",
            "epoch 400/500, loss 0.0016, auroc 1.000\n",
            "epoch 500/500, loss 0.0004, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5297865562280981\n",
            "\n",
            "3720\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(48)\n",
            "fail:  tensor(141)\n",
            "epoch 100/500, loss 0.0030, auroc 1.000\n",
            "epoch 200/500, loss 0.0744, auroc 0.994\n",
            "epoch 300/500, loss 0.0006, auroc 1.000\n",
            "epoch 400/500, loss 0.0003, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5360520094562647\n",
            "\n",
            "187\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(48)\n",
            "fail:  tensor(141)\n",
            "epoch 100/500, loss 0.0031, auroc 1.000\n",
            "epoch 200/500, loss 0.0003, auroc 1.000\n",
            "epoch 300/500, loss 0.0161, auroc 1.000\n",
            "epoch 400/500, loss 0.0010, auroc 1.000\n",
            "epoch 500/500, loss 0.0003, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5544473995271868\n",
            "\n",
            "6442\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "succ:  tensor(47)\n",
            "fail:  tensor(142)\n",
            "epoch 100/500, loss 0.0040, auroc 1.000\n",
            "epoch 200/500, loss 0.0095, auroc 1.000\n",
            "epoch 300/500, loss 0.0006, auroc 1.000\n",
            "epoch 400/500, loss 0.0002, auroc 1.000\n",
            "epoch 500/500, loss 0.0004, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.6351513335331136\n",
            "\n"
          ]
        }
      ],
      "source": [
        "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
        "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkUELaaq3M1r",
        "outputId": "32796d94-dc50-42bb-d5d3-8abb4eac6e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:forbidden_words']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(40)\n",
            "te_fail:  tensor(62)\n",
            "tr_succ:  tensor(191)\n",
            "tr_fail:  tensor(625)\n",
            "epoch 100/500, loss 0.0202, auroc 1.000\n",
            "epoch 200/500, loss 0.0691, auroc 0.998\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0036, auroc 1.000\n",
            "epoch 500/500, loss 0.0241, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.2923387096774193\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:existence']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(27)\n",
            "te_fail:  tensor(75)\n",
            "tr_succ:  tensor(204)\n",
            "tr_fail:  tensor(612)\n",
            "epoch 100/500, loss 0.7774, auroc 0.968\n",
            "epoch 200/500, loss 1.8389, auroc 0.939\n",
            "epoch 300/500, loss 1.8384, auroc 0.940\n",
            "epoch 400/500, loss 1.8383, auroc 0.941\n",
            "epoch 500/500, loss 0.1669, auroc 0.977\n",
            "<class '__main__.LRProbe'> :  0.5338271604938272\n",
            "\n",
            "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['keywords:frequency']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(35)\n",
            "te_fail:  tensor(67)\n",
            "tr_succ:  tensor(196)\n",
            "tr_fail:  tensor(620)\n",
            "epoch 100/500, loss 0.1557, auroc 0.979\n",
            "epoch 200/500, loss 0.0002, auroc 1.000\n",
            "epoch 300/500, loss 0.0858, auroc 0.997\n",
            "epoch 400/500, loss 0.0004, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.48486140724946686\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_content:number_placeholders']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(46)\n",
            "te_fail:  tensor(56)\n",
            "tr_succ:  tensor(185)\n",
            "tr_fail:  tensor(631)\n",
            "epoch 100/500, loss 0.7199, auroc 0.956\n",
            "epoch 200/500, loss 0.0165, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0005, auroc 1.000\n",
            "epoch 500/500, loss 0.0216, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5601708074534162\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['startend:end_checker']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(14)\n",
            "te_fail:  tensor(88)\n",
            "tr_succ:  tensor(217)\n",
            "tr_fail:  tensor(599)\n",
            "epoch 100/500, loss 0.0110, auroc 1.000\n",
            "epoch 200/500, loss 0.0028, auroc 1.000\n",
            "epoch 300/500, loss 0.0627, auroc 0.999\n",
            "epoch 400/500, loss 0.0018, auroc 1.000\n",
            "epoch 500/500, loss 0.0001, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5113636363636364\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
            "['detectable_format:number_bullet_lists']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(13)\n",
            "te_fail:  tensor(89)\n",
            "tr_succ:  tensor(218)\n",
            "tr_fail:  tensor(598)\n",
            "epoch 100/500, loss 0.0103, auroc 1.000\n",
            "epoch 200/500, loss 0.0358, auroc 1.000\n",
            "epoch 300/500, loss 0.0004, auroc 1.000\n",
            "epoch 400/500, loss 0.1062, auroc 0.998\n",
            "epoch 500/500, loss 0.0005, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.5211754537597235\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
            "['length_constraints:number_words']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(54)\n",
            "te_fail:  tensor(48)\n",
            "tr_succ:  tensor(177)\n",
            "tr_fail:  tensor(639)\n",
            "epoch 100/500, loss 0.1152, auroc 0.996\n",
            "epoch 200/500, loss 0.0568, auroc 1.000\n",
            "epoch 300/500, loss 0.0022, auroc 1.000\n",
            "epoch 400/500, loss 0.0352, auroc 1.000\n",
            "epoch 500/500, loss 0.0284, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.4124228395061728\n",
            "\n",
            "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
            "['change_case:english_lowercase']\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "num_act:  918\n",
            "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
            "torch.Size([918, 2048])\n",
            "te_succ:  tensor(2)\n",
            "te_fail:  tensor(202)\n",
            "tr_succ:  tensor(229)\n",
            "tr_fail:  tensor(485)\n",
            "epoch 100/500, loss 0.0081, auroc 1.000\n",
            "epoch 200/500, loss 0.0226, auroc 1.000\n",
            "epoch 300/500, loss 0.0001, auroc 1.000\n",
            "epoch 400/500, loss 0.0000, auroc 1.000\n",
            "epoch 500/500, loss 0.0000, auroc 1.000\n",
            "<class '__main__.LRProbe'> :  0.4975247524752475\n",
            "\n",
            "keywords:forbidden_words\n",
            "keywords:existence\n",
            "keywords:frequency\n",
            "detectable_content:number_placeholders\n",
            "startend:end_checker\n",
            "detectable_format:number_bullet_lists\n",
            "length_constraints:number_words\n",
            "change_case:english_lowercase\n",
            "Total AUROC: 0.43351166058589635\n"
          ]
        }
      ],
      "source": [
        "KEYWORDS = [\\\n",
        "    'keywords:frequency',\n",
        "    'keywords:forbidden_words',\n",
        "    'keywords:existence',\n",
        "    'detectable_content:number_placeholders',\n",
        "    \"startend:end_checker\",\n",
        "    \"detectable_format:number_bullet_lists\",\n",
        "    \"length_constraints:number_words\",\n",
        "    \"change_case:english_lowercase\",\n",
        "    ]\n",
        "\n",
        "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
        "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
        "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}