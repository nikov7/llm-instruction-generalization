{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nikov7/llm-instruction-generalization.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r87fiKKQqsLC",
    "outputId": "1723eefc-8f34-44dd-a4c3-be7cb9729e0d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch as t\n",
    "import pickle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3-v73vfqsLC"
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qnfGcwuqsLD",
    "outputId": "7dbe6d21-97ba-43e0-abc5-95a068eed9d9"
   },
   "outputs": [],
   "source": [
    "def readjsonl(datapath):\n",
    "    res = []\n",
    "    with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(json.loads(line))\n",
    "    return res\n",
    "\n",
    "# // Get all detailed instructions\n",
    "def get_inst_list(task_path_ifeval):\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
    "    inst_list=[]\n",
    "    for i in instruction_id_list:\n",
    "        for j in i:\n",
    "            if j not in inst_list:\n",
    "                inst_list.append(j)\n",
    "    return inst_list\n",
    "\n",
    "# // Get all high level instructions\n",
    "def get_high_inst_list(task_path_ifeval):\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    instruction_id_list = ifeval_eval_df['instruction_id_list']\n",
    "    inst_list=[]\n",
    "    for i in instruction_id_list:\n",
    "        for j in i:\n",
    "            j = j.split(':')[0]\n",
    "            if j not in inst_list:\n",
    "                inst_list.append(j)\n",
    "    return inst_list\n",
    "\n",
    "# // Get all task type\n",
    "def get_task_list(task_path_ifeval):\n",
    "    ifeval_eval_df = pd.DataFrame(readjsonl(task_path_ifeval))\n",
    "    prompt_df = ifeval_eval_df['prompt']\n",
    "    task_list=[]\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    for prompt in prompt_df:\n",
    "        task = tokenizer.tokenize(prompt)[0]\n",
    "        if task not in task_list:\n",
    "            task_list.append(task)\n",
    "    return task_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvVE8iUzqsLD"
   },
   "source": [
    "# Linear Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MED-zH2hqsLD"
   },
   "outputs": [],
   "source": [
    "class DataModuleActIfevalSimple:\n",
    "    def __init__(self,\n",
    "                 ifeval_data_path,\n",
    "                 ifeval_eval_path,\n",
    "                 inst_list,\n",
    "                 task_list,\n",
    "                 layer=13,\n",
    "                 target_token='last',\n",
    "                 center=True,\n",
    "                 scale=False,\n",
    "                 ):\n",
    "        self.layer=layer\n",
    "\n",
    "        # // Load data\n",
    "        self.ifeval_data = self.load_response_df(ifeval_data_path)\n",
    "        ifeval_eval_df = self.load_response_df(os.path.join(ifeval_eval_path, 'eval_results_loose.jsonl'))\n",
    "\n",
    "\n",
    "        # // Select index by inst\n",
    "        inst_ind = []\n",
    "        for i in range(len(ifeval_eval_df)):\n",
    "            if ifeval_eval_df.iloc[i]['instruction_id_list'][0] in inst_list:\n",
    "                inst_ind.append(i)\n",
    "\n",
    "        # // Select index by task\n",
    "        task_ind = []\n",
    "        tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        for i in range(len(ifeval_eval_df)):\n",
    "            prompt = ifeval_eval_df.iloc[i]['prompt']\n",
    "            task = tokenizer.tokenize(prompt)[0]\n",
    "            if task in task_list:\n",
    "                task_ind.append(i)\n",
    "\n",
    "        # // Select index intersection\n",
    "        select_ind = list(set(inst_ind) & set(task_ind))\n",
    "\n",
    "        # // Load acts and labels\n",
    "        self.labels = torch.tensor(ifeval_eval_df['follow_all_instructions'])[select_ind]\n",
    "        self.labels = self.labels.float()\n",
    "        self.acts = self.collect_acts(ifeval_eval_path, layer=self.layer, target_token=target_token, device='cuda', center=center, scale=scale, index_list=select_ind)\n",
    "        self.acts = self.acts.float()\n",
    "        self.data={}\n",
    "        self.data = self.acts, self.labels\n",
    "        print('Saved layers: ', self.saved_layers)\n",
    "\n",
    "    def load_response_df(self, task_path, type='loose'):\n",
    "        response_df = pd.DataFrame(self.readjsonl(task_path))\n",
    "        return response_df\n",
    "\n",
    "    def readjsonl(self, datapath):\n",
    "        res = []\n",
    "        with open(datapath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f.readlines():\n",
    "                res.append(json.loads(line))\n",
    "        return res\n",
    "\n",
    "    def load_pickle(self, filename: str):\n",
    "        with open(filename, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def collect_acts(self, task_path, layer=13, target_token='last', device='cuda', center=True, scale=False, index_list=None):\n",
    "        \"\"\"\n",
    "        Collects activations from a dataset of statements, returns as a tensor of shape [n_activations, activation_dimension].\n",
    "        First token: [1, len_input, hidden_emb]\n",
    "        Last token: [1, 1, hidden_emb]\n",
    "        \"\"\"\n",
    "        act_path = os.path.join(task_path, \"activations\")\n",
    "        _num_act = len(os.listdir(act_path))\n",
    "        acts = []\n",
    "        print('num_act: ', _num_act)\n",
    "        for _idx in range(_num_act):\n",
    "            if index_list is not None and _idx in index_list:\n",
    "                act_file_name = os.path.join(act_path, f\"sample_{_idx}.pkl\")\n",
    "                act = self.load_pickle(act_file_name)\n",
    "                self.saved_layers = act[f'output_token_{target_token}'].keys()\n",
    "                act = act[f'output_token_{target_token}'][f'layer_{layer}']\n",
    "                act = act[:,-1] # <-- last of the first token, no problem for last token --> [1, hidden_emb]\n",
    "                acts.append(act)\n",
    "        acts = torch.cat(acts, dim=0).to(device)\n",
    "        if center:\n",
    "            acts = acts - torch.mean(acts, dim=0)\n",
    "        if scale:\n",
    "            acts = acts / torch.std(acts, dim=0)\n",
    "        return acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7jmUViTdqsLD"
   },
   "outputs": [],
   "source": [
    "\n",
    "class LRProbe(t.nn.Module):\n",
    "    def __init__(self, d_in, binary_threshold=0.5, **kwargs):\n",
    "        super().__init__()\n",
    "        self.net = t.nn.Sequential(\n",
    "            t.nn.Linear(d_in, 1, bias=False),\n",
    "            t.nn.Sigmoid()\n",
    "        )\n",
    "        self.binary_threshold = binary_threshold\n",
    "\n",
    "    def forward(self, x, iid=None):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def pred(self, x, iid=None, binary_threshold=None):\n",
    "        binary_threshold = binary_threshold if binary_threshold is not None else self.binary_threshold\n",
    "        return (self(x)>binary_threshold).float()\n",
    "\n",
    "    def probability(self, x, iid=None):\n",
    "        return self(x)\n",
    "\n",
    "    def from_data(acts, labels, lr=0.001, weight_decay=0.1, epochs=1000, device='cpu', class_weight_one=None, **kwargs):\n",
    "        acts, labels = acts.to(device), labels.to(device)\n",
    "        probe = LRProbe(acts.shape[-1]).to(device)\n",
    "\n",
    "        opt = t.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        for _ in range(epochs):\n",
    "            opt.zero_grad()\n",
    "            if class_weight_one is not None:\n",
    "                class_weight = torch.ones_like(labels)\n",
    "                class_weight[labels>0] = class_weight_one\n",
    "                loss = t.nn.BCELoss(weight=class_weight)(probe(acts), labels)\n",
    "            else:\n",
    "                loss = t.nn.BCELoss()(probe(acts), labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        return probe\n",
    "\n",
    "    @property\n",
    "    def direction(self):\n",
    "        return self.net[0].weight.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTivbrumqsLE"
   },
   "source": [
    "# 1. Task generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task_generalization(ifeval_data_path, task_path_ifeval):\n",
    "    roc_list=[]\n",
    "    m_roc_list=[]\n",
    "    seed_list = np.random.randint(0, 10000, 5)\n",
    "    for seed in seed_list:\n",
    "        print(seed)\n",
    "    \n",
    "        # // Select train and test task\n",
    "        task_list = np.array(get_task_list(ifeval_data_path))\n",
    "        torch.manual_seed(seed)\n",
    "        split=0.8\n",
    "        train_ind_list = torch.randperm(len(task_list)) < int(split * len(task_list))\n",
    "        test_ind_list = ~train_ind_list\n",
    "        train_task_list = task_list[train_ind_list]\n",
    "        test_task_list = task_list[test_ind_list]\n",
    "    \n",
    "        # // Use all instructions\n",
    "        inst_list = get_inst_list(ifeval_data_path)\n",
    "    \n",
    "        # // Get train data\n",
    "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, train_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, test_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        train_acts, train_labels = train_dm.data\n",
    "        test_acts, test_labels = test_dm.data\n",
    "    \n",
    "        # // Scale and Center\n",
    "        all_acts = torch.cat((train_acts, test_acts))\n",
    "        print(all_acts.shape)\n",
    "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "    \n",
    "        # // Stat of test\n",
    "        succ = (test_labels==1).sum()\n",
    "        fail = (test_labels==0).sum()\n",
    "        print('succ: ', succ)\n",
    "        print('fail: ', fail)\n",
    "    \n",
    "    \n",
    "        # // Train probe\n",
    "        max_roc=0\n",
    "        probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5)\n",
    "    \n",
    "        # // Test\n",
    "        test_prob = probe.probability(test_acts).detach().cpu()\n",
    "        auroc = roc_auc_score(test_labels, test_prob)\n",
    "        roc_list.append(auroc)\n",
    "    \n",
    "        print(LRProbe, ': ', auroc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER=14\n",
    "MODEL='TinyLlama-1.1B-Chat-v1.0'\n",
    "TOKEN='first'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 ifeval_simple (v1, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3167\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(33)\n",
      "fail:  tensor(72)\n",
      "<class '__main__.LRProbe'> :  0.6593013468013468\n",
      "\n",
      "3857\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(31)\n",
      "fail:  tensor(74)\n",
      "<class '__main__.LRProbe'> :  0.6556233653007847\n",
      "\n",
      "2416\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(38)\n",
      "fail:  tensor(67)\n",
      "<class '__main__.LRProbe'> :  0.6441476826394344\n",
      "\n",
      "7642\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(32)\n",
      "fail:  tensor(73)\n",
      "<class '__main__.LRProbe'> :  0.7367294520547946\n",
      "\n",
      "7372\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(46)\n",
      "fail:  tensor(59)\n",
      "<class '__main__.LRProbe'> :  0.6879145173176123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
    "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ifeval_simple_v2 (3 new tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3729\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(44)\n",
      "fail:  tensor(124)\n",
      "<class '__main__.LRProbe'> :  0.5706561583577713\n",
      "\n",
      "6878\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(57)\n",
      "fail:  tensor(111)\n",
      "<class '__main__.LRProbe'> :  0.5730994152046784\n",
      "\n",
      "7122\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(48)\n",
      "fail:  tensor(120)\n",
      "<class '__main__.LRProbe'> :  0.5078125\n",
      "\n",
      "2143\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(47)\n",
      "fail:  tensor(121)\n",
      "<class '__main__.LRProbe'> :  0.5657640232108317\n",
      "\n",
      "4523\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(48)\n",
      "fail:  tensor(120)\n",
      "<class '__main__.LRProbe'> :  0.5498263888888889\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
    "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ifeval_simple_v3 (3 new tasks+combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4806\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(46)\n",
      "fail:  tensor(143)\n",
      "<class '__main__.LRProbe'> :  0.5799635147461235\n",
      "\n",
      "5161\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(57)\n",
      "fail:  tensor(132)\n",
      "<class '__main__.LRProbe'> :  0.583599149388623\n",
      "\n",
      "2413\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(52)\n",
      "fail:  tensor(137)\n",
      "<class '__main__.LRProbe'> :  0.592363840539023\n",
      "\n",
      "1951\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(37)\n",
      "fail:  tensor(152)\n",
      "<class '__main__.LRProbe'> :  0.5545874822190612\n",
      "\n",
      "916\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(49)\n",
      "fail:  tensor(140)\n",
      "<class '__main__.LRProbe'> :  0.47536443148688046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
    "evaluate_task_generalization(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGWFvQJbqsLE"
   },
   "source": [
    "# 2. Intruction generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inst_generalization(ifeval_data_path, task_path_ifeval, keyword_list):\n",
    "    # // Make a dict for result\n",
    "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
    "    re={}\n",
    "    all_label={}\n",
    "    all_pred={}\n",
    "    for i in inst_list:\n",
    "        if i not in re.keys():\n",
    "            re[i]=[]\n",
    "            all_label[i]=[]\n",
    "            all_pred[i]=[]\n",
    "    roc_list=[]\n",
    "    total_pred=[]\n",
    "    total_label=[]\n",
    "    \n",
    "    # // Use all task\n",
    "    task_list = get_task_list(ifeval_data_path)\n",
    "    \n",
    "    # // Select train and test inst\n",
    "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
    "    \n",
    "    final={}\n",
    "    for inst in inst_list:\n",
    "        final[inst]=[]\n",
    "    \n",
    "    for inst in inst_list:\n",
    "    \n",
    "        # // Leave one out\n",
    "        train_inst_list = [i for i in keyword_list if i != inst]\n",
    "        test_inst_list = [inst]\n",
    "        print(train_inst_list)\n",
    "        print(test_inst_list)\n",
    "    \n",
    "        # // Get train data\n",
    "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, train_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, test_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        train_acts, train_labels = train_dm.data\n",
    "        test_acts, test_labels = test_dm.data\n",
    "    \n",
    "        # // Scale and Center\n",
    "        all_acts = torch.cat((train_acts, test_acts))\n",
    "        print(all_acts.shape)\n",
    "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "    \n",
    "        # // Stat of test\n",
    "        succ = (test_labels==1).sum()\n",
    "        fail = (test_labels==0).sum()\n",
    "        print('te_succ: ', succ)\n",
    "        print('te_fail: ', fail)\n",
    "    \n",
    "        # // Stat of train\n",
    "        tr_succ = (train_labels==1).sum()\n",
    "        tr_fail = (train_labels==0).sum()\n",
    "        print('tr_succ: ', tr_succ)\n",
    "        print('tr_fail: ', tr_fail)\n",
    "        tr_class_weight = tr_succ/tr_fail\n",
    "    \n",
    "        # // exception\n",
    "        if succ<1 or fail<1:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "        # // Train probe\n",
    "        probe = LRProbe.from_data(train_acts, train_labels, device='cuda', epochs=1000, binary_threshold=0.5, class_weight_one=None)\n",
    "    \n",
    "        # // Test\n",
    "        test_prob = probe.probability(test_acts).detach().cpu()\n",
    "        auroc = roc_auc_score(test_labels, test_prob)\n",
    "    \n",
    "        print(LRProbe, ': ', auroc)\n",
    "        print()\n",
    "    \n",
    "        # // save\n",
    "        roc_list.append(auroc)\n",
    "        re[inst].append(auroc)\n",
    "        all_label[inst].append(test_labels)\n",
    "        all_pred[inst].append(test_prob)\n",
    "        total_label.append(test_labels)\n",
    "        total_pred.append(test_prob)\n",
    "    \n",
    "    for key in all_pred.keys():\n",
    "        if len(all_pred[key])>0:\n",
    "            print(key)\n",
    "            label = np.concatenate(all_label[key])\n",
    "            pred = np.concatenate(all_pred[key])\n",
    "            final[key].append(roc_auc_score(label, pred ))\n",
    "    \n",
    "    # // Compute all auc total\n",
    "    label = np.concatenate(total_label)\n",
    "    pred = np.concatenate(total_pred)\n",
    "    total_auroc = roc_auc_score(label, pred)\n",
    "    print(f\"Total AUROC: {total_auroc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 ifeval_simple (v1, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Kc4UC6p6aXiU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(122)\n",
      "tr_fail:  tensor(286)\n",
      "<class '__main__.LRProbe'> :  0.4959677419354839\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:existence']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(135)\n",
      "tr_fail:  tensor(273)\n",
      "<class '__main__.LRProbe'> :  0.5520987654320988\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:frequency']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(127)\n",
      "tr_fail:  tensor(281)\n",
      "<class '__main__.LRProbe'> :  0.49339019189765465\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(116)\n",
      "tr_fail:  tensor(292)\n",
      "<class '__main__.LRProbe'> :  0.46059782608695654\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders']\n",
      "['startend:end_checker']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(148)\n",
      "tr_fail:  tensor(260)\n",
      "<class '__main__.LRProbe'> :  0.5125811688311688\n",
      "\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "Total AUROC: 0.4984390520788988\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\"\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
    "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ifeval_simple_v2 (3 new tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(183)\n",
      "tr_fail:  tensor(531)\n",
      "<class '__main__.LRProbe'> :  0.47298387096774186\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:existence']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(196)\n",
      "tr_fail:  tensor(518)\n",
      "<class '__main__.LRProbe'> :  0.4795061728395062\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:frequency']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(188)\n",
      "tr_fail:  tensor(526)\n",
      "<class '__main__.LRProbe'> :  0.5599147121535182\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(177)\n",
      "tr_fail:  tensor(537)\n",
      "<class '__main__.LRProbe'> :  0.46855590062111807\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['startend:end_checker']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(209)\n",
      "tr_fail:  tensor(505)\n",
      "<class '__main__.LRProbe'> :  0.48133116883116883\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_format:number_bullet_lists']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(13)\n",
      "te_fail:  tensor(89)\n",
      "tr_succ:  tensor(210)\n",
      "tr_fail:  tensor(504)\n",
      "<class '__main__.LRProbe'> :  0.4848746758859118\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
      "['length_constraints:number_words']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(48)\n",
      "te_fail:  tensor(54)\n",
      "tr_succ:  tensor(175)\n",
      "tr_fail:  tensor(539)\n",
      "<class '__main__.LRProbe'> :  0.5374228395061729\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
      "['change_case:english_lowercase']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(0)\n",
      "te_fail:  tensor(102)\n",
      "tr_succ:  tensor(223)\n",
      "tr_fail:  tensor(491)\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "detectable_format:number_bullet_lists\n",
      "length_constraints:number_words\n",
      "Total AUROC: 0.4971916012895801\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\",\n",
    "    \"detectable_format:number_bullet_lists\",\n",
    "    \"length_constraints:number_words\",\n",
    "    \"change_case:english_lowercase\"\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
    "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ifeval_simple_v3 (3 new tasks+combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(191)\n",
      "tr_fail:  tensor(625)\n",
      "<class '__main__.LRProbe'> :  0.5044354838709677\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:existence']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(204)\n",
      "tr_fail:  tensor(612)\n",
      "<class '__main__.LRProbe'> :  0.4780246913580247\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:frequency']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(196)\n",
      "tr_fail:  tensor(620)\n",
      "<class '__main__.LRProbe'> :  0.5053304904051172\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(185)\n",
      "tr_fail:  tensor(631)\n",
      "<class '__main__.LRProbe'> :  0.5069875776397516\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['startend:end_checker']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(217)\n",
      "tr_fail:  tensor(599)\n",
      "<class '__main__.LRProbe'> :  0.5008116883116883\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_format:number_bullet_lists']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(13)\n",
      "te_fail:  tensor(89)\n",
      "tr_succ:  tensor(218)\n",
      "tr_fail:  tensor(598)\n",
      "<class '__main__.LRProbe'> :  0.5570440795159897\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
      "['length_constraints:number_words']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(54)\n",
      "te_fail:  tensor(48)\n",
      "tr_succ:  tensor(177)\n",
      "tr_fail:  tensor(639)\n",
      "<class '__main__.LRProbe'> :  0.470679012345679\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
      "['change_case:english_lowercase']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(2)\n",
      "te_fail:  tensor(202)\n",
      "tr_succ:  tensor(229)\n",
      "tr_fail:  tensor(485)\n",
      "<class '__main__.LRProbe'> :  0.693069306930693\n",
      "\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "detectable_format:number_bullet_lists\n",
      "length_constraints:number_words\n",
      "change_case:english_lowercase\n",
      "Total AUROC: 0.4996912354990958\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\",\n",
    "    \"detectable_format:number_bullet_lists\",\n",
    "    \"length_constraints:number_words\",\n",
    "    \"change_case:english_lowercase\",\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
    "evaluate_inst_generalization(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MLP Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPProbe(t.nn.Module):\n",
    "    \"\"\"\n",
    "    A small multi‑layer perceptron probe:\n",
    "      - input dim → hidden_dim → hidden_dim → 1 → sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, d_in, hidden_dim=512, n_hidden=2, binary_threshold=0.5, dropout=0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = d_in\n",
    "        for _ in range(n_hidden):\n",
    "            layers += [\n",
    "                t.nn.Linear(in_dim, hidden_dim),\n",
    "                t.nn.ReLU(),\n",
    "                t.nn.Dropout(dropout),\n",
    "            ]\n",
    "            in_dim = hidden_dim\n",
    "        layers += [t.nn.Linear(in_dim, 1, bias=False), t.nn.Sigmoid()]\n",
    "        self.net = t.nn.Sequential(*layers)\n",
    "        self.binary_threshold = binary_threshold\n",
    "\n",
    "    def forward(self, x, iid=None):\n",
    "        return self.net(x).squeeze(-1)\n",
    "\n",
    "    def pred(self, x, iid=None, binary_threshold=None):\n",
    "        thresh = binary_threshold if binary_threshold is not None else self.binary_threshold\n",
    "        return (self(x) > thresh).float()\n",
    "\n",
    "    def probability(self, x, iid=None):\n",
    "        return self(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def from_data(acts, labels,\n",
    "                  lr=1e-3,\n",
    "                  weight_decay=1e-2,\n",
    "                  epochs=500,\n",
    "                  device='cpu',\n",
    "                  hidden_dim=512,\n",
    "                  n_hidden=2,\n",
    "                  dropout=0.1):\n",
    "        \"\"\"\n",
    "        Train an MLPProbe on (acts, labels) and return the fitted probe.\n",
    "        \"\"\"\n",
    "        acts, labels = acts.to(device), labels.to(device)\n",
    "        probe = MLPProbe(acts.shape[-1], hidden_dim, n_hidden, dropout=dropout).to(device)\n",
    "        optimizer = t.optim.AdamW(probe.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        loss_fn = t.nn.BCELoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            preds = probe(acts)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # optional: print every 100 iters\n",
    "            if (epoch + 1) % 100 == 0:\n",
    "                with torch.no_grad():\n",
    "                    auc = roc_auc_score(labels.cpu().numpy(), preds.detach().cpu().numpy())\n",
    "                print(f\"epoch {epoch+1}/{epochs}, loss {loss.item():.4f}, auroc {auc:.3f}\")\n",
    "        return probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_task_generalization_mlp(ifeval_data_path, task_path_ifeval):\n",
    "    roc_list=[]\n",
    "    m_roc_list=[]\n",
    "    seed_list = np.random.randint(0, 10000, 5)\n",
    "    for seed in seed_list:\n",
    "        print(seed)\n",
    "    \n",
    "        # // Select train and test task\n",
    "        task_list = np.array(get_task_list(ifeval_data_path))\n",
    "        torch.manual_seed(seed)\n",
    "        split=0.8\n",
    "        train_ind_list = torch.randperm(len(task_list)) < int(split * len(task_list))\n",
    "        test_ind_list = ~train_ind_list\n",
    "        train_task_list = task_list[train_ind_list]\n",
    "        test_task_list = task_list[test_ind_list]\n",
    "    \n",
    "        # // Use all instructions\n",
    "        inst_list = get_inst_list(ifeval_data_path)\n",
    "    \n",
    "        # // Get train data\n",
    "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, train_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, inst_list, test_task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        train_acts, train_labels = train_dm.data\n",
    "        test_acts, test_labels = test_dm.data\n",
    "    \n",
    "        # // Scale and Center\n",
    "        all_acts = torch.cat((train_acts, test_acts))\n",
    "        print(all_acts.shape)\n",
    "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "    \n",
    "        # // Stat of test\n",
    "        succ = (test_labels==1).sum()\n",
    "        fail = (test_labels==0).sum()\n",
    "        print('succ: ', succ)\n",
    "        print('fail: ', fail)\n",
    "    \n",
    "    \n",
    "        # // Train probe\n",
    "        max_roc=0\n",
    "\n",
    "        probe = MLPProbe.from_data(\n",
    "            train_acts, train_labels,\n",
    "            device='cuda',\n",
    "            lr=0.001,\n",
    "            weight_decay=0.01,\n",
    "            epochs=500,\n",
    "            hidden_dim=512,\n",
    "            n_hidden=2,\n",
    "            dropout=0.1\n",
    "        )\n",
    "    \n",
    "        # // Test\n",
    "        test_prob = probe.probability(test_acts).detach().cpu()\n",
    "        auroc = roc_auc_score(test_labels, test_prob)\n",
    "        roc_list.append(auroc)\n",
    "    \n",
    "        print(LRProbe, ': ', auroc)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_inst_generalization_mlp(ifeval_data_path, task_path_ifeval, keyword_list):\n",
    "    # // Make a dict for result\n",
    "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
    "    re={}\n",
    "    all_label={}\n",
    "    all_pred={}\n",
    "    for i in inst_list:\n",
    "        if i not in re.keys():\n",
    "            re[i]=[]\n",
    "            all_label[i]=[]\n",
    "            all_pred[i]=[]\n",
    "    roc_list=[]\n",
    "    total_pred=[]\n",
    "    total_label=[]\n",
    "    \n",
    "    # // Use all task\n",
    "    task_list = get_task_list(ifeval_data_path)\n",
    "    \n",
    "    # // Select train and test inst\n",
    "    inst_list = np.array(get_inst_list(ifeval_data_path))\n",
    "    \n",
    "    final={}\n",
    "    for inst in inst_list:\n",
    "        final[inst]=[]\n",
    "    \n",
    "    for inst in inst_list:\n",
    "    \n",
    "        # // Leave one out\n",
    "        train_inst_list = [i for i in keyword_list if i != inst]\n",
    "        test_inst_list = [inst]\n",
    "        print(train_inst_list)\n",
    "        print(test_inst_list)\n",
    "    \n",
    "        # // Get train data\n",
    "        train_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, train_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        test_dm = DataModuleActIfevalSimple(ifeval_data_path, task_path_ifeval, test_inst_list, task_list, layer=LAYER, target_token=TOKEN, center=True, scale=True)\n",
    "        train_acts, train_labels = train_dm.data\n",
    "        test_acts, test_labels = test_dm.data\n",
    "    \n",
    "        # // Scale and Center\n",
    "        all_acts = torch.cat((train_acts, test_acts))\n",
    "        print(all_acts.shape)\n",
    "        train_acts = train_acts - torch.mean(train_acts, dim=0)\n",
    "        train_acts = train_acts / torch.std(train_acts, dim=0)\n",
    "        test_acts = test_acts - torch.mean(train_acts, dim=0)\n",
    "        test_acts = test_acts / torch.std(train_acts, dim=0)\n",
    "    \n",
    "        # // Stat of test\n",
    "        succ = (test_labels==1).sum()\n",
    "        fail = (test_labels==0).sum()\n",
    "        print('te_succ: ', succ)\n",
    "        print('te_fail: ', fail)\n",
    "    \n",
    "        # // Stat of train\n",
    "        tr_succ = (train_labels==1).sum()\n",
    "        tr_fail = (train_labels==0).sum()\n",
    "        print('tr_succ: ', tr_succ)\n",
    "        print('tr_fail: ', tr_fail)\n",
    "        tr_class_weight = tr_succ/tr_fail\n",
    "    \n",
    "        # // exception\n",
    "        if succ<1 or fail<1:\n",
    "            continue\n",
    "    \n",
    "    \n",
    "        # // Train probe\n",
    "        probe = MLPProbe.from_data(\n",
    "            train_acts, train_labels,\n",
    "            device='cuda',\n",
    "            lr=0.001,\n",
    "            weight_decay=0.01,\n",
    "            epochs=500,           \n",
    "            hidden_dim=512,\n",
    "            n_hidden=4,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # // Test\n",
    "        test_prob = probe.probability(test_acts).detach().cpu()\n",
    "        auroc = roc_auc_score(test_labels, test_prob)\n",
    "    \n",
    "        print(LRProbe, ': ', auroc)\n",
    "        print()\n",
    "    \n",
    "        # // save\n",
    "        roc_list.append(auroc)\n",
    "        re[inst].append(auroc)\n",
    "        all_label[inst].append(test_labels)\n",
    "        all_pred[inst].append(test_prob)\n",
    "        total_label.append(test_labels)\n",
    "        total_pred.append(test_prob)\n",
    "    \n",
    "    for key in all_pred.keys():\n",
    "        if len(all_pred[key])>0:\n",
    "            print(key)\n",
    "            label = np.concatenate(all_label[key])\n",
    "            pred = np.concatenate(all_pred[key])\n",
    "            final[key].append(roc_auc_score(label, pred ))\n",
    "    \n",
    "    # // Compute all auc total\n",
    "    label = np.concatenate(total_label)\n",
    "    pred = np.concatenate(total_pred)\n",
    "    total_auroc = roc_auc_score(label, pred)\n",
    "    print(f\"Total AUROC: {total_auroc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ifeval_simple (v1, original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8845\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(28)\n",
      "fail:  tensor(77)\n",
      "epoch 100/500, loss 0.0015, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.7500000000000001\n",
      "\n",
      "7829\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(43)\n",
      "fail:  tensor(62)\n",
      "epoch 100/500, loss 0.0006, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.689047261815454\n",
      "\n",
      "6161\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(35)\n",
      "fail:  tensor(70)\n",
      "epoch 100/500, loss 0.0013, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.6248979591836735\n",
      "\n",
      "5264\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(25)\n",
      "fail:  tensor(80)\n",
      "epoch 100/500, loss 0.0008, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.712\n",
      "\n",
      "8272\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "succ:  tensor(29)\n",
      "fail:  tensor(76)\n",
      "epoch 100/500, loss 0.0010, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0024, auroc 1.000\n",
      "epoch 400/500, loss 0.0009, auroc 1.000\n",
      "epoch 500/500, loss 0.0004, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.691016333938294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
    "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(122)\n",
      "tr_fail:  tensor(286)\n",
      "epoch 100/500, loss 0.0037, auroc 1.000\n",
      "epoch 200/500, loss 0.0001, auroc 1.000\n",
      "epoch 300/500, loss 0.2509, auroc 0.967\n",
      "epoch 400/500, loss 0.0486, auroc 0.998\n",
      "epoch 500/500, loss 0.2300, auroc 0.962\n",
      "<class '__main__.LRProbe'> :  0.43306451612903224\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:existence']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(135)\n",
      "tr_fail:  tensor(273)\n",
      "epoch 100/500, loss 0.2265, auroc 0.998\n",
      "epoch 200/500, loss 0.0016, auroc 1.000\n",
      "epoch 300/500, loss 0.0021, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.4849382716049383\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker']\n",
      "['keywords:frequency']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(127)\n",
      "tr_fail:  tensor(281)\n",
      "epoch 100/500, loss 0.0425, auroc 1.000\n",
      "epoch 200/500, loss 0.0000, auroc 1.000\n",
      "epoch 300/500, loss 0.0000, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5872068230277186\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(116)\n",
      "tr_fail:  tensor(292)\n",
      "epoch 100/500, loss 0.0024, auroc 1.000\n",
      "epoch 200/500, loss 0.0000, auroc 1.000\n",
      "epoch 300/500, loss 0.0000, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.4343944099378882\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders']\n",
      "['startend:end_checker']\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  510\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([510, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(148)\n",
      "tr_fail:  tensor(260)\n",
      "epoch 100/500, loss 0.2582, auroc 0.981\n",
      "epoch 200/500, loss 0.0036, auroc 1.000\n",
      "epoch 300/500, loss 0.0040, auroc 1.000\n",
      "epoch 400/500, loss 0.0002, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5604707792207793\n",
      "\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "Total AUROC: 0.491130977721016\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\"\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v1.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v1\"\n",
    "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ifeval_simple_v2 (3 new tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7866\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(51)\n",
      "fail:  tensor(117)\n",
      "epoch 100/500, loss 0.0013, auroc 1.000\n",
      "epoch 200/500, loss 0.0913, auroc 0.992\n",
      "epoch 300/500, loss 0.0013, auroc 1.000\n",
      "epoch 400/500, loss 0.0003, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5760013407072231\n",
      "\n",
      "5527\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(51)\n",
      "fail:  tensor(117)\n",
      "epoch 100/500, loss 0.0027, auroc 1.000\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.583375230434054\n",
      "\n",
      "9141\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(42)\n",
      "fail:  tensor(126)\n",
      "epoch 100/500, loss 0.0034, auroc 1.000\n",
      "epoch 200/500, loss 0.0003, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.6375661375661377\n",
      "\n",
      "4289\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(38)\n",
      "fail:  tensor(130)\n",
      "epoch 100/500, loss 0.0029, auroc 1.000\n",
      "epoch 200/500, loss 0.0006, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 12.3668, auroc 0.683\n",
      "<class '__main__.LRProbe'> :  0.4779352226720648\n",
      "\n",
      "5831\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "succ:  tensor(45)\n",
      "fail:  tensor(123)\n",
      "epoch 100/500, loss 0.5142, auroc 0.963\n",
      "epoch 200/500, loss 0.0014, auroc 1.000\n",
      "epoch 300/500, loss 0.0004, auroc 1.000\n",
      "epoch 400/500, loss 0.0002, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.6445347786811201\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
    "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(183)\n",
      "tr_fail:  tensor(531)\n",
      "epoch 100/500, loss 0.0885, auroc 0.997\n",
      "epoch 200/500, loss 0.0007, auroc 1.000\n",
      "epoch 300/500, loss 0.0033, auroc 1.000\n",
      "epoch 400/500, loss 0.0002, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.4721774193548387\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:existence']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(196)\n",
      "tr_fail:  tensor(518)\n",
      "epoch 100/500, loss 0.0189, auroc 1.000\n",
      "epoch 200/500, loss 0.0006, auroc 1.000\n",
      "epoch 300/500, loss 0.0003, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0050, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5802469135802468\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:frequency']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(188)\n",
      "tr_fail:  tensor(526)\n",
      "epoch 100/500, loss 0.0236, auroc 1.000\n",
      "epoch 200/500, loss 0.0003, auroc 1.000\n",
      "epoch 300/500, loss 0.0561, auroc 0.999\n",
      "epoch 400/500, loss 0.1073, auroc 0.992\n",
      "epoch 500/500, loss 0.0024, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5543710021321961\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(177)\n",
      "tr_fail:  tensor(537)\n",
      "epoch 100/500, loss 0.0021, auroc 1.000\n",
      "epoch 200/500, loss 0.0013, auroc 1.000\n",
      "epoch 300/500, loss 0.0008, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.48990683229813664\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['startend:end_checker']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(209)\n",
      "tr_fail:  tensor(505)\n",
      "epoch 100/500, loss 0.2139, auroc 0.993\n",
      "epoch 200/500, loss 0.0012, auroc 1.000\n",
      "epoch 300/500, loss 0.0954, auroc 0.997\n",
      "epoch 400/500, loss 0.0055, auroc 1.000\n",
      "epoch 500/500, loss 0.0007, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5714285714285714\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_format:number_bullet_lists']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(13)\n",
      "te_fail:  tensor(89)\n",
      "tr_succ:  tensor(210)\n",
      "tr_fail:  tensor(504)\n",
      "epoch 100/500, loss 0.0773, auroc 0.996\n",
      "epoch 200/500, loss 0.0001, auroc 1.000\n",
      "epoch 300/500, loss 0.0054, auroc 1.000\n",
      "epoch 400/500, loss 0.0001, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5432152117545377\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
      "['length_constraints:number_words']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(48)\n",
      "te_fail:  tensor(54)\n",
      "tr_succ:  tensor(175)\n",
      "tr_fail:  tensor(539)\n",
      "epoch 100/500, loss 0.0423, auroc 1.000\n",
      "epoch 200/500, loss 0.0242, auroc 1.000\n",
      "epoch 300/500, loss 0.0000, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5802469135802469\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
      "['change_case:english_lowercase']\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  816\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([816, 2048])\n",
      "te_succ:  tensor(0)\n",
      "te_fail:  tensor(102)\n",
      "tr_succ:  tensor(223)\n",
      "tr_fail:  tensor(491)\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "detectable_format:number_bullet_lists\n",
      "length_constraints:number_words\n",
      "Total AUROC: 0.4898806316385522\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\",\n",
    "    \"detectable_format:number_bullet_lists\",\n",
    "    \"length_constraints:number_words\",\n",
    "    \"change_case:english_lowercase\"\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v2.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v2\"\n",
    "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ifeval_simple_v3 (3 new tasks+combination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2116\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(51)\n",
      "fail:  tensor(138)\n",
      "epoch 100/500, loss 0.0043, auroc 1.000\n",
      "epoch 200/500, loss 0.0357, auroc 1.000\n",
      "epoch 300/500, loss 0.0026, auroc 1.000\n",
      "epoch 400/500, loss 0.0006, auroc 1.000\n",
      "epoch 500/500, loss 0.0003, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.6445012787723785\n",
      "\n",
      "9502\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(43)\n",
      "fail:  tensor(146)\n",
      "epoch 100/500, loss 0.0030, auroc 1.000\n",
      "epoch 200/500, loss 0.0003, auroc 1.000\n",
      "epoch 300/500, loss 0.1338, auroc 0.987\n",
      "epoch 400/500, loss 0.0016, auroc 1.000\n",
      "epoch 500/500, loss 0.0004, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5297865562280981\n",
      "\n",
      "3720\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(48)\n",
      "fail:  tensor(141)\n",
      "epoch 100/500, loss 0.0030, auroc 1.000\n",
      "epoch 200/500, loss 0.0744, auroc 0.994\n",
      "epoch 300/500, loss 0.0006, auroc 1.000\n",
      "epoch 400/500, loss 0.0003, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5360520094562647\n",
      "\n",
      "187\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(48)\n",
      "fail:  tensor(141)\n",
      "epoch 100/500, loss 0.0031, auroc 1.000\n",
      "epoch 200/500, loss 0.0003, auroc 1.000\n",
      "epoch 300/500, loss 0.0161, auroc 1.000\n",
      "epoch 400/500, loss 0.0010, auroc 1.000\n",
      "epoch 500/500, loss 0.0003, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5544473995271868\n",
      "\n",
      "6442\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "succ:  tensor(47)\n",
      "fail:  tensor(142)\n",
      "epoch 100/500, loss 0.0040, auroc 1.000\n",
      "epoch 200/500, loss 0.0095, auroc 1.000\n",
      "epoch 300/500, loss 0.0006, auroc 1.000\n",
      "epoch 400/500, loss 0.0002, auroc 1.000\n",
      "epoch 500/500, loss 0.0004, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.6351513335331136\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
    "evaluate_task_generalization_mlp(DATA_PATH, TASK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keywords:frequency', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:forbidden_words']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(40)\n",
      "te_fail:  tensor(62)\n",
      "tr_succ:  tensor(191)\n",
      "tr_fail:  tensor(625)\n",
      "epoch 100/500, loss 0.0202, auroc 1.000\n",
      "epoch 200/500, loss 0.0691, auroc 0.998\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0036, auroc 1.000\n",
      "epoch 500/500, loss 0.0241, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.2923387096774193\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:existence']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(27)\n",
      "te_fail:  tensor(75)\n",
      "tr_succ:  tensor(204)\n",
      "tr_fail:  tensor(612)\n",
      "epoch 100/500, loss 0.7774, auroc 0.968\n",
      "epoch 200/500, loss 1.8389, auroc 0.939\n",
      "epoch 300/500, loss 1.8384, auroc 0.940\n",
      "epoch 400/500, loss 1.8383, auroc 0.941\n",
      "epoch 500/500, loss 0.1669, auroc 0.977\n",
      "<class '__main__.LRProbe'> :  0.5338271604938272\n",
      "\n",
      "['keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['keywords:frequency']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(35)\n",
      "te_fail:  tensor(67)\n",
      "tr_succ:  tensor(196)\n",
      "tr_fail:  tensor(620)\n",
      "epoch 100/500, loss 0.1557, auroc 0.979\n",
      "epoch 200/500, loss 0.0002, auroc 1.000\n",
      "epoch 300/500, loss 0.0858, auroc 0.997\n",
      "epoch 400/500, loss 0.0004, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.48486140724946686\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_content:number_placeholders']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(46)\n",
      "te_fail:  tensor(56)\n",
      "tr_succ:  tensor(185)\n",
      "tr_fail:  tensor(631)\n",
      "epoch 100/500, loss 0.7199, auroc 0.956\n",
      "epoch 200/500, loss 0.0165, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0005, auroc 1.000\n",
      "epoch 500/500, loss 0.0216, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5601708074534162\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'detectable_format:number_bullet_lists', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['startend:end_checker']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(14)\n",
      "te_fail:  tensor(88)\n",
      "tr_succ:  tensor(217)\n",
      "tr_fail:  tensor(599)\n",
      "epoch 100/500, loss 0.0110, auroc 1.000\n",
      "epoch 200/500, loss 0.0028, auroc 1.000\n",
      "epoch 300/500, loss 0.0627, auroc 0.999\n",
      "epoch 400/500, loss 0.0018, auroc 1.000\n",
      "epoch 500/500, loss 0.0001, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5113636363636364\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'length_constraints:number_words', 'change_case:english_lowercase']\n",
      "['detectable_format:number_bullet_lists']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(13)\n",
      "te_fail:  tensor(89)\n",
      "tr_succ:  tensor(218)\n",
      "tr_fail:  tensor(598)\n",
      "epoch 100/500, loss 0.0103, auroc 1.000\n",
      "epoch 200/500, loss 0.0358, auroc 1.000\n",
      "epoch 300/500, loss 0.0004, auroc 1.000\n",
      "epoch 400/500, loss 0.1062, auroc 0.998\n",
      "epoch 500/500, loss 0.0005, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.5211754537597235\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'change_case:english_lowercase']\n",
      "['length_constraints:number_words']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(54)\n",
      "te_fail:  tensor(48)\n",
      "tr_succ:  tensor(177)\n",
      "tr_fail:  tensor(639)\n",
      "epoch 100/500, loss 0.1152, auroc 0.996\n",
      "epoch 200/500, loss 0.0568, auroc 1.000\n",
      "epoch 300/500, loss 0.0022, auroc 1.000\n",
      "epoch 400/500, loss 0.0352, auroc 1.000\n",
      "epoch 500/500, loss 0.0284, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.4124228395061728\n",
      "\n",
      "['keywords:frequency', 'keywords:forbidden_words', 'keywords:existence', 'detectable_content:number_placeholders', 'startend:end_checker', 'detectable_format:number_bullet_lists', 'length_constraints:number_words']\n",
      "['change_case:english_lowercase']\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "num_act:  918\n",
      "Saved layers:  dict_keys(['layer_22', 'layer_18', 'layer_14'])\n",
      "torch.Size([918, 2048])\n",
      "te_succ:  tensor(2)\n",
      "te_fail:  tensor(202)\n",
      "tr_succ:  tensor(229)\n",
      "tr_fail:  tensor(485)\n",
      "epoch 100/500, loss 0.0081, auroc 1.000\n",
      "epoch 200/500, loss 0.0226, auroc 1.000\n",
      "epoch 300/500, loss 0.0001, auroc 1.000\n",
      "epoch 400/500, loss 0.0000, auroc 1.000\n",
      "epoch 500/500, loss 0.0000, auroc 1.000\n",
      "<class '__main__.LRProbe'> :  0.4975247524752475\n",
      "\n",
      "keywords:forbidden_words\n",
      "keywords:existence\n",
      "keywords:frequency\n",
      "detectable_content:number_placeholders\n",
      "startend:end_checker\n",
      "detectable_format:number_bullet_lists\n",
      "length_constraints:number_words\n",
      "change_case:english_lowercase\n",
      "Total AUROC: 0.43351166058589635\n"
     ]
    }
   ],
   "source": [
    "KEYWORDS = [\\\n",
    "    'keywords:frequency',\n",
    "    'keywords:forbidden_words',\n",
    "    'keywords:existence',\n",
    "    'detectable_content:number_placeholders',\n",
    "    \"startend:end_checker\",\n",
    "    \"detectable_format:number_bullet_lists\",\n",
    "    \"length_constraints:number_words\",\n",
    "    \"change_case:english_lowercase\",\n",
    "    ]\n",
    "\n",
    "DATA_PATH = 'data/ifeval_simple_v3.jsonl'\n",
    "TASK_PATH = f\"data/{MODEL}/ifeval_simple_v3\"\n",
    "evaluate_inst_generalization_mlp(DATA_PATH, TASK_PATH, KEYWORDS)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
